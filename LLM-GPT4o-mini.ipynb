{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification using LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/deceptive-opinion-spam-corpus/deceptive-opinion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = train_df.sample(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_prompt(n_shots=3):\n",
    "    \"\"\"\n",
    "    Generate a few-shot prompt by randomly selecting n examples from the training data.\n",
    "    Ensure that the number of truthful and deceptive examples are equal, and handle odd n_shots.\n",
    "    Formats the prompt in a user/assistant conversation format.\n",
    "\n",
    "    Args:\n",
    "        n_shots (int): Number of examples to include in the prompt\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt with few-shot examples in ChatGPT conversation format\n",
    "    \"\"\"\n",
    "    # Randomly sample n examples from training data\n",
    "    # Sample equal numbers of truthful and deceptive reviews, handling odd n_shots\n",
    "    n_per_class = n_shots // 2\n",
    "    remaining = n_shots % 2\n",
    "\n",
    "    # Get base samples for each class\n",
    "    deceptive = train_df[train_df[\"deceptive\"] == \"deceptive\"].sample(n=n_per_class)\n",
    "    truthful = train_df[train_df[\"deceptive\"] == \"truthful\"].sample(n=n_per_class)\n",
    "\n",
    "    # If n_shots is odd, randomly add one more example from either class\n",
    "    if remaining:\n",
    "        extra_sample = train_df.sample(n=1)\n",
    "        few_shot_examples = pd.concat([deceptive, truthful, extra_sample]).sample(\n",
    "            frac=1\n",
    "        )\n",
    "    else:\n",
    "        few_shot_examples = pd.concat([deceptive, truthful]).sample(frac=1)\n",
    "\n",
    "    few_shot_prompts = []\n",
    "    # Add the few-shot examples in conversation format\n",
    "    for _, example in few_shot_examples.iterrows():\n",
    "        few_shot_prompts.append({\"role\": \"user\", \"content\": f\"{example['text']}\"})\n",
    "        few_shot_prompts.append(\n",
    "            {\"role\": \"assistant\", \"content\": f\"{example['deceptive']}\"}\n",
    "        )\n",
    "\n",
    "    return few_shot_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, model=\"gpt-4o-mini\", n_shots=0):\n",
    "    \"\"\"\n",
    "    Get a response from the OpenAI API for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text message to be classified\n",
    "        model (str, optional): The OpenAI model to use. Defaults to \"gpt-4o-mini\"\n",
    "        n_shots (int, optional): Number of examples to include. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's classification response (ideally 'truthful' or 'deceptive')\n",
    "    \"\"\"\n",
    "\n",
    "    system_role = f\"\"\"You are a classification model trained to identify whether incoming messages are spam. \n",
    "    Given a message, analyze its content, structure, and intent to determine if it is 'truthful' (not spam) or 'deceptive' (spam). \n",
    "    Return 'truthful' if the message does not contain signs of spam and is a genuine communication. \n",
    "    Return 'deceptive' if the message displays characteristics commonly associated with spam, such as misleading claims, requests for sensitive information, or excessive promotions.\"\"\"\n",
    "\n",
    "    # Build messages list starting with system role\n",
    "    messages = [{\"role\": \"system\", \"content\": system_role}]\n",
    "\n",
    "    # Add example messages based on n_shots\n",
    "    if n_shots > 0:\n",
    "        # Get few-shot examples - In-Context Learning\n",
    "        context = get_few_shot_prompt(n_shots)\n",
    "        # Add pairs of examples up to n_shots\n",
    "        messages.extend(context)\n",
    "\n",
    "    # Add the actual prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    completion = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_predictions(result):\n",
    "    \"\"\"\n",
    "    Count occurrences of each class and check for invalid predictions.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each class\n",
    "    deceptive_count = result.count(\"deceptive\")\n",
    "    truthful_count = result.count(\"truthful\")\n",
    "\n",
    "    # Print the counts\n",
    "    print(f\"Total predictions: {len(result)}\")\n",
    "    print(f\"Number of deceptive predictions: {deceptive_count}\")\n",
    "    print(f\"Number of truthful predictions: {truthful_count}\")\n",
    "    print(\n",
    "        f\"Number of invalid predictions: {len(result) - deceptive_count - truthful_count}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Check for any invalid predictions\n",
    "    for i in result:\n",
    "        if i != \"deceptive\" and i != \"truthful\":\n",
    "            print(f\"Invalid prediction found: \\n{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_true, y_pred, result):\n",
    "    \"\"\"\n",
    "    Get classification report.\n",
    "    \"\"\"\n",
    "    # Filter out invalid predictions\n",
    "    for pred in result:\n",
    "        if pred in [\"deceptive\", \"truthful\"]:\n",
    "            y_pred.append(pred)\n",
    "        else:\n",
    "            # For invalid predictions, we'll count them as incorrect by using the opposite of true label\n",
    "            true_idx = len(y_pred)  # Get index to find corresponding true label\n",
    "            y_pred.append(\n",
    "                \"truthful\" if y_true[true_idx] == \"deceptive\" else \"deceptive\"\n",
    "            )\n",
    "\n",
    "    # Print classification report with all metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Get confusion matrix and visualize it.\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix using sklearn\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[\"deceptive\", \"truthful\"])\n",
    "\n",
    "    # Create heatmap visualization of confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        xticklabels=[\"deceptive\", \"truthful\"],\n",
    "        yticklabels=[\"deceptive\", \"truthful\"],\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(result, n_shots):\n",
    "    \"\"\"\n",
    "    Save results to a CSV file.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with test data and predictions\n",
    "    results_df = test_df.copy()\n",
    "    results_df[\"predicted\"] = result\n",
    "\n",
    "    # Save to CSV file\n",
    "    results_df.to_csv(f\"./results/llm/prediction_results_{n_shots}.csv\", index=False)\n",
    "    print(f\"Results saved to prediction_results_{n_shots}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get response from LLM and store in result\n",
    "# result = []\n",
    "\n",
    "# for text in tqdm(test_df.text):\n",
    "#     response = get_response(text)\n",
    "#     result.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save result to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_results(result, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_predictions(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: LLM response containing \"truthful\" or \"deceptive\" may be considered as respective valid predictions, but for the purpose of evaluation, we will count them as invalid here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions and actual values to lists\n",
    "y_true = list(test_df.deceptive)\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_classification_report(result)\n",
    "# get_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"deceptive\" class, 71% of the times the model predicted \"deceptive\", it was correct; 29% of the times it was incorrect. For \"truthful\" class, 50% of the times the model predicted \"truthful\", it was correct; 50% of the times it was incorrect.\n",
    "\n",
    "For \"deceptive\" class, only 1% of actual deceptive cases were correctly identified, and 99% of deceptive cases were misclassified as truthful. For \"truthful\" class, 100% of actual truthful cases were correctly identified, but it was not meaningful since the model predicted \"truthful\" for most cases (99.56%).\n",
    "\n",
    "The f1-score for \"deceptive\" class is 0.01, which is very low, although the f1-score for \"truthful\" class is 0.67, again it is not meaningful for the reason mentioned above.\n",
    "\n",
    "In conclusion, the statistics above show that the LLM model is not able to distinguish between deceptive and truthful messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_results(n_shots):\n",
    "    \"\"\"\n",
    "    Helper function to pipeline the process of getting few-shot results.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each text in the test dataset\n",
    "    for text in tqdm(test_df.text):\n",
    "        response = get_response(text, n_shots=n_shots)\n",
    "        result.append(response)\n",
    "\n",
    "    # Save results to csv file\n",
    "    save_results(result, n_shots)\n",
    "\n",
    "    # Count and print predictions\n",
    "    count_predictions(result)\n",
    "\n",
    "    # Convert predictions and actual values to lists\n",
    "    y_true = list(test_df.deceptive)\n",
    "    y_pred = []\n",
    "\n",
    "    # Get classification report\n",
    "    get_classification_report(y_true, y_pred, result)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    get_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for text in tqdm(test_df.text):\n",
    "    response = get_response(text, n_shots=1)\n",
    "    result.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_predictions(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions and actual values to lists\n",
    "y_true = list(test_df.deceptive)\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_report(y_true, y_pred, result)\n",
    "get_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_few_shot_results(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_few_shot_results(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_few_shot_results(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
